{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(r\"C:\\Users\\sm225\\OneDrive\\Desktop\\science_exam\\llm_Science_exam\\dataset\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and Checkpoint for the LLM\n",
    "model_checkpoint = '/kaggle/input/huggingface-bert/bert-large-cased'\n",
    "batch_size = 2\n",
    "\n",
    "# Set up the list of options\n",
    "options = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Define a mapping of string values to integer values\n",
    "mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "reverse_mapping = {0: 'A', 1:'B', 2:'C', 3:'D', 4:'E'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the values in the column using the defined mapping\n",
    "df_data['answer'] = df_data['answer'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the Pandas dataframe into Dataset\n",
    "datasets = Dataset.from_pandas(df_data)\n",
    "\n",
    "datasets = datasets.rename_column(\"answer\", \"label\")\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'prompt': 'Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?',\n",
       " 'A': 'MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"',\n",
       " 'B': 'MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.',\n",
       " 'C': 'MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.',\n",
       " 'D': 'MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.',\n",
       " 'E': 'MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a dataset record from Dataset object\n",
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to randomly display some dataset from the Dataset object\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=1):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "# to show a question, choices and correct answer\n",
    "def show_one(example):\n",
    "    print(f\"prompt: {example['prompt']}\")\n",
    "    print(f\"  A - {example['A']}\")\n",
    "    print(f\"  B - {example['B']}\")\n",
    "    print(f\"  C - {example['C']}\")\n",
    "    print(f\"  D - {example['D']}\")\n",
    "    print(f\"  E - {example['E']}\")\n",
    "    print(f\"Ground truth: option {example['label']}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>What are the Navier-Stokes equations?</td>\n",
       "      <td>The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.</td>\n",
       "      <td>The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for non-Newtonian fluids.</td>\n",
       "      <td>The Navier-Stokes equations are partial differential equations that describe the motion of non-viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.</td>\n",
       "      <td>The Navier-Stokes equations are algebraic equations that describe the motion of non-viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.</td>\n",
       "      <td>The Navier-Stokes equations are algebraic equations that describe the motion of viscous fluid substances, expressing momentum balance and conservation of mass for Newtonian fluids.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>What is the significance of probability amplitudes in quantum mechanics?</td>\n",
       "      <td>Probability amplitudes are used to determine the mass of particles in quantum mechanics.</td>\n",
       "      <td>Probability amplitudes have no significance in quantum mechanics.</td>\n",
       "      <td>Probability amplitudes are used to determine the velocity of particles in quantum mechanics.</td>\n",
       "      <td>Probability amplitudes act as the equivalent of conventional probabilities in classical mechanics, with many analogous laws.</td>\n",
       "      <td>Probability amplitudes act as the equivalent of conventional probabilities in quantum mechanics, with many analogous laws.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156</td>\n",
       "      <td>What is the Evans balance?</td>\n",
       "      <td>The Evans balance is a system used to measure the change in weight of a sample when an electromagnet is turned on, which is proportional to the susceptibility.</td>\n",
       "      <td>The Evans balance is a system used to measure the dependence of the NMR frequency of a liquid sample on its shape or orientation to determine its susceptibility.</td>\n",
       "      <td>The Evans balance is a system used to measure the magnetic field distortion around a sample immersed in water inside an MR scanner to determine its susceptibility.</td>\n",
       "      <td>The Evans balance is a system used to measure the susceptibility of a sample by measuring the force change on a strong compact magnet upon insertion of the sample.</td>\n",
       "      <td>The Evans balance is a system used to measure the magnetic susceptibility of most crystals, which is not a scalar quantity.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets, num_examples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "  A - MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
      "  B - MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
      "  C - MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
      "  D - MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
      "  E - MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
      "Ground truth: option 3\n"
     ]
    }
   ],
   "source": [
    "show_one(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1203eb5ad6aa424d95a51493d68d119a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sm225\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sm225\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0824,  0.0667, -0.2880,  ..., -0.3566,  0.1960,  0.5381],\n",
      "         [ 0.0310, -0.1448,  0.0952,  ..., -0.1560,  1.0151,  0.0947],\n",
      "         [-0.8935,  0.3240,  0.4184,  ..., -0.5498,  0.2853,  0.1149],\n",
      "         ...,\n",
      "         [-0.2812, -0.8531,  0.6912,  ..., -0.5051,  0.4716, -0.6854],\n",
      "         [-0.4429, -0.7820, -0.8055,  ...,  0.1949,  0.1081,  0.0130],\n",
      "         [ 0.5570, -0.1080, -0.2412,  ...,  0.2817, -0.3996, -0.1882]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9397, -0.4081, -0.9024,  0.8667,  0.6076, -0.1782,  0.9319,  0.2685,\n",
      "         -0.7918, -1.0000, -0.4899,  0.9625,  0.9823,  0.6102,  0.9614, -0.8728,\n",
      "         -0.6449, -0.6543,  0.3102, -0.6648,  0.7556,  1.0000,  0.0778,  0.3350,\n",
      "          0.5094,  0.9948, -0.8847,  0.9590,  0.9761,  0.7384, -0.7787,  0.1232,\n",
      "         -0.9912, -0.2119, -0.9225, -0.9931,  0.3767, -0.8050, -0.0945,  0.0497,\n",
      "         -0.9269,  0.2934,  1.0000,  0.0954,  0.4904, -0.3206, -1.0000,  0.2759,\n",
      "         -0.9303,  0.9396,  0.9162,  0.8490,  0.1967,  0.4833,  0.5391,  0.0523,\n",
      "         -0.0621,  0.1046, -0.3403, -0.6018, -0.6203,  0.5439, -0.8909, -0.9180,\n",
      "          0.9392,  0.8156, -0.1900, -0.2589, -0.1371, -0.1553,  0.9367,  0.3183,\n",
      "          0.2468, -0.9167,  0.5823,  0.2861, -0.7534,  1.0000, -0.7911, -0.9841,\n",
      "          0.8615,  0.7741,  0.6616, -0.2575,  0.4217, -1.0000,  0.6669, -0.1115,\n",
      "         -0.9931,  0.1993,  0.6389, -0.3458,  0.3185,  0.6436, -0.6661, -0.4977,\n",
      "         -0.3677, -0.8909, -0.3536, -0.3582,  0.1323, -0.2020, -0.5409, -0.4236,\n",
      "          0.3375, -0.5047, -0.6625,  0.3016, -0.1905,  0.7032,  0.4906, -0.3784,\n",
      "          0.5841, -0.9767,  0.6868, -0.4095, -0.9919, -0.6602, -0.9917,  0.7231,\n",
      "         -0.4432, -0.3315,  0.9772, -0.0030,  0.5428, -0.1079, -0.9419, -1.0000,\n",
      "         -0.8208, -0.7195, -0.1495, -0.3632, -0.9843, -0.9785,  0.6604,  0.9671,\n",
      "          0.1991,  0.9999, -0.3270,  0.9683, -0.2824, -0.6671,  0.6429, -0.5084,\n",
      "          0.8155,  0.6683, -0.7370,  0.1841, -0.3255,  0.5425, -0.8196, -0.1444,\n",
      "         -0.7372, -0.9632, -0.4311,  0.9677, -0.5258, -0.9448,  0.0683, -0.2570,\n",
      "         -0.6629,  0.8783,  0.8162,  0.4679, -0.4237,  0.4789,  0.3930,  0.6878,\n",
      "         -0.9157, -0.1113,  0.5141, -0.3371, -0.8507, -0.9828, -0.4308,  0.6103,\n",
      "          0.9940,  0.8002,  0.3561,  0.8153, -0.2915,  0.8075, -0.9715,  0.9865,\n",
      "         -0.2685,  0.2925,  0.0508,  0.4006, -0.9311,  0.0247,  0.8974, -0.6805,\n",
      "         -0.8900, -0.2348, -0.5269, -0.4351, -0.8138,  0.6458, -0.2967, -0.2653,\n",
      "         -0.1253,  0.9394,  0.9895,  0.8081,  0.2221,  0.8180, -0.9162, -0.5713,\n",
      "          0.0396,  0.3294,  0.1492,  0.9960, -0.7699, -0.1358, -0.9500, -0.9895,\n",
      "         -0.0589, -0.9379, -0.2007, -0.7227,  0.7361,  0.1222,  0.5693,  0.5728,\n",
      "         -0.9927, -0.7939,  0.3953, -0.5294,  0.4758, -0.2439,  0.6086,  0.9451,\n",
      "         -0.6614,  0.8475,  0.9563, -0.8955, -0.8207,  0.8418, -0.3480,  0.9098,\n",
      "         -0.7207,  0.9900,  0.9613,  0.9021, -0.9444, -0.7457, -0.9020, -0.7801,\n",
      "         -0.0605,  0.1208,  0.9016,  0.6999,  0.4337,  0.3760, -0.7694,  0.9992,\n",
      "         -0.6057, -0.9718, -0.4017, -0.1144, -0.9907,  0.9075,  0.3436,  0.3461,\n",
      "         -0.4579, -0.7868, -0.9760,  0.9109,  0.1416,  0.9900, -0.1437, -0.9715,\n",
      "         -0.5963, -0.9475, -0.2035, -0.2663, -0.2594, -0.1224, -0.9710,  0.5504,\n",
      "          0.5669,  0.6306, -0.7672,  0.9996,  1.0000,  0.9747,  0.9272,  0.9433,\n",
      "         -0.9999, -0.6034,  1.0000, -0.9959, -1.0000, -0.9484, -0.7638,  0.4284,\n",
      "         -1.0000, -0.2423, -0.0149, -0.9346,  0.7198,  0.9799,  0.9970, -1.0000,\n",
      "          0.8571,  0.9582, -0.7485,  0.9826, -0.5256,  0.9773,  0.5886,  0.3242,\n",
      "         -0.3219,  0.3844, -0.9395, -0.9149, -0.5524, -0.7445,  0.9986,  0.2265,\n",
      "         -0.8418, -0.9291,  0.3760, -0.2026, -0.3773, -0.9807, -0.2517,  0.6540,\n",
      "          0.8830,  0.1926,  0.3737, -0.7180,  0.3621, -0.0460,  0.2923,  0.7520,\n",
      "         -0.9328, -0.4846, -0.3400, -0.3362, -0.6643, -0.9751,  0.9772, -0.4883,\n",
      "          0.9261,  1.0000,  0.1624, -0.9297,  0.7345,  0.2806, -0.3267,  1.0000,\n",
      "          0.8273, -0.9825, -0.6232,  0.5221, -0.6021, -0.6570,  0.9997, -0.1715,\n",
      "         -0.6950, -0.5274,  0.9871, -0.9913,  0.9965, -0.9464, -0.9779,  0.9795,\n",
      "          0.9524, -0.8368, -0.7578,  0.2502, -0.7006,  0.2992, -0.9726,  0.8163,\n",
      "          0.6003, -0.1536,  0.9077, -0.9091, -0.6684,  0.2876, -0.7330, -0.1607,\n",
      "          0.9486,  0.6312, -0.3891,  0.0357, -0.3750, -0.3756, -0.9843,  0.4886,\n",
      "          1.0000, -0.2315,  0.7898, -0.4258, -0.0598, -0.1328,  0.5125,  0.6314,\n",
      "         -0.3294, -0.8755,  0.7849, -0.9849, -0.9887,  0.8402,  0.1892, -0.4375,\n",
      "          1.0000,  0.5749,  0.1210,  0.4122,  0.9909,  0.0735,  0.7185,  0.8889,\n",
      "          0.9896, -0.2700,  0.6525,  0.9098, -0.9027, -0.4159, -0.7008, -0.0415,\n",
      "         -0.9281, -0.0108, -0.9747,  0.9768,  0.9692,  0.4282,  0.2692,  0.6122,\n",
      "          1.0000, -0.5248,  0.7249, -0.5060,  0.9194, -0.9998, -0.9260, -0.4517,\n",
      "         -0.1464, -0.8025, -0.4218,  0.4552, -0.9794,  0.8356,  0.6287, -0.9972,\n",
      "         -0.9938, -0.2074,  0.9298,  0.1215, -0.9909, -0.8170, -0.6627,  0.6416,\n",
      "         -0.2284, -0.9569, -0.1181, -0.3816,  0.5255, -0.1805,  0.6387,  0.8702,\n",
      "          0.6870, -0.5757, -0.4236, -0.1641, -0.8831,  0.9374, -0.8926, -0.9400,\n",
      "         -0.3102,  1.0000, -0.6176,  0.9136,  0.7730,  0.7931, -0.0889,  0.2407,\n",
      "          0.9472,  0.3261, -0.7425, -0.8732, -0.7849, -0.4377,  0.8024,  0.3961,\n",
      "          0.7152,  0.8478,  0.8273,  0.1153, -0.0280, -0.0304,  0.9999, -0.2497,\n",
      "         -0.1616, -0.5044, -0.0497, -0.4457, -0.5473,  1.0000,  0.3189,  0.5105,\n",
      "         -0.9929, -0.9074, -0.9470,  1.0000,  0.8645, -0.8185,  0.7758,  0.6998,\n",
      "         -0.1522,  0.8929, -0.2187, -0.4045,  0.3307,  0.1409,  0.9588, -0.6211,\n",
      "         -0.9823, -0.7474,  0.4825, -0.9754,  1.0000, -0.6499, -0.2883, -0.5406,\n",
      "         -0.1761,  0.3477, -0.1121, -0.9871, -0.2494,  0.3136,  0.9691,  0.3786,\n",
      "         -0.6823, -0.9428,  0.7981,  0.8634, -0.9307, -0.9558,  0.9710, -0.9912,\n",
      "          0.6371,  1.0000,  0.3606,  0.0868,  0.2708, -0.5297,  0.4133, -0.4391,\n",
      "          0.8383, -0.9726, -0.3193, -0.2312,  0.3197, -0.1520, -0.2309,  0.8155,\n",
      "          0.2249, -0.6628, -0.7201, -0.0532,  0.4476,  0.9189, -0.2601, -0.2309,\n",
      "          0.1324, -0.1847, -0.9499, -0.2363, -0.5325, -1.0000,  0.7762, -1.0000,\n",
      "          0.5303,  0.2326, -0.2700,  0.9060,  0.2731,  0.5734, -0.8361, -0.8113,\n",
      "          0.5569,  0.8021, -0.3536, -0.6216, -0.7758,  0.4175, -0.0477,  0.1000,\n",
      "         -0.5561,  0.7914, -0.2158,  1.0000,  0.2179, -0.8865, -0.9858,  0.1659,\n",
      "         -0.3391,  1.0000, -0.9481, -0.9695,  0.5338, -0.8202, -0.8819,  0.4277,\n",
      "          0.0594, -0.8486, -0.9633,  0.9736,  0.9464, -0.6571,  0.4920, -0.3911,\n",
      "         -0.6615,  0.0692,  0.8727,  0.9897,  0.4700,  0.9277,  0.6218, -0.2349,\n",
      "          0.9815,  0.2672,  0.5438,  0.0797,  1.0000,  0.3870, -0.9405,  0.1054,\n",
      "         -0.9833, -0.2969, -0.9634,  0.3935,  0.2849,  0.9189, -0.2147,  0.9719,\n",
      "         -0.8230,  0.0331, -0.7876, -0.4855,  0.4738, -0.9335, -0.9865, -0.9889,\n",
      "          0.6081, -0.4843, -0.0682,  0.1664,  0.1078,  0.4640,  0.4898, -1.0000,\n",
      "          0.9641,  0.5431,  0.9383,  0.9721,  0.7680,  0.5263,  0.3403, -0.9916,\n",
      "         -0.9905, -0.3485, -0.2593,  0.7984,  0.6853,  0.9404,  0.5173, -0.5142,\n",
      "         -0.1765, -0.4202, -0.4569, -0.9952,  0.5883, -0.5553, -0.9812,  0.9735,\n",
      "         -0.3406, -0.1781, -0.0011, -0.8343,  0.9537,  0.8525,  0.4713, -0.0046,\n",
      "          0.5341,  0.9080,  0.9707,  0.9846, -0.8114,  0.9081, -0.6079,  0.5477,\n",
      "          0.8132, -0.9515,  0.0974,  0.5627, -0.5420,  0.3376, -0.2709, -0.9814,\n",
      "          0.7589, -0.3889,  0.6699, -0.4872,  0.0730, -0.4788, -0.0787, -0.7815,\n",
      "         -0.8119,  0.6705,  0.6675,  0.9399,  0.7869, -0.0141, -0.7990, -0.1767,\n",
      "         -0.8166, -0.9326,  0.9578, -0.0282, -0.3473,  0.6796, -0.0607,  0.8229,\n",
      "          0.1077, -0.3942, -0.3221, -0.7277,  0.8907, -0.5713, -0.5725, -0.6333,\n",
      "          0.7569,  0.3500,  1.0000, -0.7834, -0.9078, -0.4479, -0.3861,  0.5302,\n",
      "         -0.7148, -1.0000,  0.4726, -0.4242,  0.7148, -0.7536,  0.8473, -0.7694,\n",
      "         -0.9885, -0.3057,  0.5318,  0.7787, -0.4794, -0.6866,  0.6466, -0.1783,\n",
      "          0.9834,  0.9262, -0.6138,  0.2273,  0.6907, -0.7303, -0.7535,  0.9454]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Specify a valid model checkpoint\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "try:\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Test the tokenizer and model\n",
    "    text = \"Hello, how are you?\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    print(outputs)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 1037, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use the Tokenizer to transform the text into token representation\n",
    "tokenizer(\"Hello, this is a sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Repeat each prompt for 5 times to go with the 5 possibilities of each option\n",
    "    first_sentences = [[context] * 5 for context in examples[\"prompt\"]]\n",
    "    # Grab all options\n",
    "\n",
    "    second_sentences = [examples[options[i]] for i in range(5)]\n",
    "#     print(first_sentences)\n",
    "#     print(\"====\")\n",
    "\n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "    \n",
    "#     print(first_sentences)\n",
    "#     print(\"====\")\n",
    "#     print(second_sentences)\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, padding = False, truncation=True)\n",
    "    # Un-flatten\n",
    "    return {\n",
    "        k: [v[i : i + 5] for i in range(0, len(v), 5)]\n",
    "        for k, v in tokenized_examples.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'label'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take samples to validate the Tokenizer\n",
    "\n",
    "examples = datasets[:1]\n",
    "selected_dataset = Dataset.from_dict(examples)\n",
    "selected_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bbe9d4e11c4d43a5e71c954b628ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of items: 1\n",
      "No of items in the 1st item 5\n",
      "No. of tokens for each item in the 1st item [74, 82, 79, 82, 75]\n"
     ]
    }
   ],
   "source": [
    "# Use the Map funciton to tokenize the input text\n",
    "samples = selected_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print('No of items: {}'.format(len(samples[\"input_ids\"])))\n",
    "print('No of items in the 1st item {}'.format(len(samples[\"input_ids\"][0])))\n",
    "print('No. of tokens for each item in the 1st item {}'.format([len(x) for x in samples[\"input_ids\"][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sm225\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] mond is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \" fuzzy dark matter. \" [SEP]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] mond is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20. [SEP]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] mond is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions. [SEP]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] mond is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2. [SEP]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] mond is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter. [SEP]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check we didn't do anything wrong when grouping all possibilites and unflattening them, let's have a look at the decoded inputs for a given example:\n",
    "idx = 0\n",
    "[tokenizer.decode(samples[\"input_ids\"][idx][i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d8818877084030aa948644d6a3a51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, it is the time to tokenize the completed Dataset\n",
    "encoded_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sm225\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMultipleChoice.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForMultipleChoice were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, TFAutoModelForMultipleChoice\n",
    "\n",
    "model = TFAutoModelForMultipleChoice.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# push_to_hub_model_id = f\"{model_name}-finetuned-swag\"\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "learning_rate = 0.00001\n",
    "batch_size = batch_size\n",
    "num_train_epochs = 10\n",
    "weight_decay = 0.001\n",
    "report_to=None\n",
    "token=False\n",
    "push_to_hub=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pad the input datasets in order to transform the input datasets with the same size \n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PaddingStrategy,\n",
    ")\n",
    "from typing import Optional, Union\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n",
    "            for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        # Un-flatten\n",
    "        batch = {\n",
    "            k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()\n",
    "        }\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "features = [{k: v for k, v in encoded_datasets[i].items() if k in accepted_keys} for i in range(200)]\n",
    "# features = [{k: v for k, v in samples[i].items() if k in accepted_keys} for i in range(5)]\n",
    "features\n",
    "batch = DataCollatorForMultipleChoice(tokenizer)(features)\n",
    "# batch = DataCollatorForMultipleChoice(tokenizer)(encoded_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] mond is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \" fuzzy dark matter. \" [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] dynamic scaling refers to the evolution of self - similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. this similarity is tested by a certain time - dependent stochastic variable x. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] the triskeles symbol was reconstructed as a feminine divine triad by the rulers of syracuse, and later adopted as an emblem. its usage may also be related to the greek name of sicily, trinacria, which means \" having three headlands. \" the head of medusa at the center of the sicilian triskeles represents the three headlands. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] regularizing the mass - energy of an electron with a finite radius can theoretically simplify calculations involving infinities or singularities, thereby providing explanations that would otherwise be impossible to achieve. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] which of the following statements accurately describes the impact of modified newtonian dynamics ( mond ) on the observed \" missing baryonic mass \" discrepancy in galaxy clusters? [SEP] the angular spacing of features in the diffraction pattern is indirectly proportional to the dimensions of the object causing the diffraction. therefore, if the diffracting object is smaller, the resulting diffraction pattern will be narrower. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(batch[\"input_ids\"][0][i].numpy().tolist()) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n",
      "  A - MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n",
      "  B - MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.\n",
      "  C - MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.\n",
      "  D - MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.\n",
      "  E - MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n",
      "Ground truth: option 3\n"
     ]
    }
   ],
   "source": [
    "show_one(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForMultipleChoice(tokenizer)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "encoded_datasets_2 = encoded_datasets.train_test_split(test_size=0.1, seed=42)  # Adjust test_size as needed\n",
    "\n",
    "# Prepare train and validation datasets using prepare_tf_dataset\n",
    "train_set = model.prepare_tf_dataset(\n",
    "    encoded_datasets_2['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "val_set = model.prepare_tf_dataset(\n",
    "    encoded_datasets_2['test'],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "all_set = model.prepare_tf_dataset(\n",
    "    encoded_datasets,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "from transformers import create_optimizer\n",
    "\n",
    "total_train_steps = (len(train_set) // batch_size) * num_train_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_warmup_steps=0, num_train_steps=total_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\sm225\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\sm225\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "100/100 [==============================] - 222s 2s/step - loss: 1.6070 - accuracy: 0.2550\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 189s 2s/step - loss: 1.6069 - accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 187s 2s/step - loss: 1.6034 - accuracy: 0.2550\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 189s 2s/step - loss: 1.6214 - accuracy: 0.1850\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 189s 2s/step - loss: 1.6028 - accuracy: 0.2350\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 188s 2s/step - loss: 1.5976 - accuracy: 0.2400\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 188s 2s/step - loss: 1.6014 - accuracy: 0.2450\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 188s 2s/step - loss: 1.6016 - accuracy: 0.2150\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 188s 2s/step - loss: 1.6063 - accuracy: 0.2350\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 196s 2s/step - loss: 1.6002 - accuracy: 0.2050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a3e41ac610>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# tensorboard_callback = TensorBoard(log_dir=\"./mc_model_save/logs\")\n",
    "\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"./mc_model_save\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     token=False,\n",
    "#     hub_model_id=push_to_hub_model_id,\n",
    "# )\n",
    "\n",
    "# callbacks = [tensorboard_callback, push_to_hub_callback]\n",
    "\n",
    "model.fit(\n",
    "    all_set,\n",
    "#     validation_data=val_set,\n",
    "    epochs=num_train_epochs,\n",
    "#     epochs=3,\n",
    "#     callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 60s 537ms/step\n"
     ]
    }
   ],
   "source": [
    "# Now we can actually make predictions on our questions\n",
    "predictions = model.predict(all_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predictions_to_map_output(predictions):\n",
    "    sorted_answer_indices = np.argsort(-predictions)\n",
    "    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n",
    "#     print(top_answer_indices.map(reverse_mapping))\n",
    "#     print(top_answer_indices)\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    top_answers = np.vectorize(reverse_mapping.get)(top_answer_indices)\n",
    "    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E A B', 'A E D', 'D E A', 'B D A', 'A D C', 'B E A', 'D A E',\n",
       "       'C E A', 'C E B', 'B A D', 'C D E', 'D B E', 'D B E', 'B A C',\n",
       "       'C A B', 'E B D', 'C D B', 'B D E', 'D E B', 'C E B', 'A E C',\n",
       "       'C A D', 'D E B', 'C D B', 'E B C', 'D B C', 'A D C', 'A E D',\n",
       "       'A C B', 'C A D', 'A B C', 'E D B', 'B E A', 'D E C', 'C A D',\n",
       "       'A B D', 'A D C', 'A E C', 'D B C', 'D B C', 'D E C', 'A B C',\n",
       "       'C E A', 'A D B', 'E A B', 'D C A', 'C A D', 'C E B', 'C A B',\n",
       "       'C D B', 'A E C', 'C A D', 'C A E', 'D C A', 'D A C', 'A E C',\n",
       "       'E B A', 'A D E', 'B E A', 'C B E', 'A B D', 'B A C', 'C A E',\n",
       "       'A E C', 'D B A', 'E D A', 'D B A', 'B E D', 'A B D', 'B C D',\n",
       "       'B D A', 'E D C', 'A D E', 'D E A', 'B C E', 'D A C', 'E D C',\n",
       "       'D A B', 'D B A', 'D B A', 'D C B', 'E A C', 'B D C', 'D B A',\n",
       "       'D E A', 'A B D', 'B A E', 'A B C', 'B D E', 'C A E', 'E B C',\n",
       "       'B D E', 'E A B', 'E A D', 'A D B', 'D B C', 'B D C', 'D C A',\n",
       "       'E B D', 'B D A', 'B D A', 'C D E', 'B E C', 'E C B', 'B A E',\n",
       "       'E B D', 'C D A', 'D E B', 'B A E', 'C B A', 'A C D', 'A D B',\n",
       "       'D E C', 'C D B', 'E B A', 'E D B', 'B E C', 'D C B', 'C D A',\n",
       "       'A B C', 'A C D', 'A B D', 'D E C', 'C B A', 'B A E', 'C D A',\n",
       "       'B E C', 'B C E', 'A B C', 'B C E', 'D B A', 'D A E', 'B D C',\n",
       "       'C E D', 'A E D', 'D E A', 'C E D', 'C D A', 'E B C', 'B E A',\n",
       "       'B E A', 'B C D', 'E C D', 'C A D', 'C B A', 'C E D', 'D B C',\n",
       "       'D A B', 'E D B', 'C D E', 'A E B', 'E C A', 'D A E', 'C D B',\n",
       "       'E B C', 'E D A', 'D B E', 'D A B', 'B E C', 'A B E', 'D B C',\n",
       "       'B C E', 'D B C', 'C D B', 'B E D', 'D A B', 'C B E', 'E B D',\n",
       "       'C E B', 'B E A', 'A C B', 'D A B', 'C E B', 'E C D', 'B C E',\n",
       "       'C E A', 'A E C', 'D E A', 'C E D', 'C D E', 'B D A', 'C A D',\n",
       "       'A B D', 'D E B', 'D B E', 'E B A', 'B D C', 'C D A', 'E B A',\n",
       "       'A C D', 'E A C', 'E C B', 'C D A', 'B C A', 'E D B', 'D B C',\n",
       "       'C A B', 'D A B', 'E A B', 'A D C'], dtype='<U5')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's double check our output looks correct:\n",
    "predictions_to_map_output(predictions.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "\n",
    "first_sentence1 = [\"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed missing baryonic mass discrepancy in galaxy clusters?\", \"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed missing baryonic mass discrepancy in galaxy clusters?\", \"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed missing baryonic mass discrepancy in galaxy clusters?\", \"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed missing baryonic mass discrepancy in galaxy clusters?\", \"Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed missing baryonic mass discrepancy in galaxy clusters?\"]\n",
    "second_sentence1 = [\n",
    "    'MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.',\n",
    "    'MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20',\n",
    "    'MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.',\n",
    "    'MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2',\n",
    "    'MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence2 = ['Which of the following is an accurate definition of dynamic scaling in self-similar systems?', 'Which of the following is an accurate definition of dynamic scaling in self-similar systems?', 'Which of the following is an accurate definition of dynamic scaling in self-similar systems?', 'Which of the following is an accurate definition of dynamic scaling in self-similar systems?', 'Which of the following is an accurate definition of dynamic scaling in self-similar systems?']\n",
    "\n",
    "second_sentence2 = [\n",
    "    'Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.',\n",
    "    'Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is similar to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x.',\n",
    "    'Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.', \n",
    "    'Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.', \n",
    "    'Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is independent of the respective data taken from snapshots of any earlier or later time. This independence is tested by a certain time-dependent stochastic variable z.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Test Case 1 =======\n",
      "Question: Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed missing baryonic mass discrepancy in galaxy clusters?\n",
      "\n",
      "The answer is choice 4: MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.\n"
     ]
    }
   ],
   "source": [
    "first = first_sentence1\n",
    "second = second_sentence1\n",
    "\n",
    "# tokenized = tokenizer(full_sentences, padding=\"longest\", return_tensors=\"np\")\n",
    "tokenized = tokenizer(first, second, padding=\"longest\", return_tensors=\"np\")\n",
    "tokenized_1 = {key: np.expand_dims(array, 0) for key, array in tokenized.items()}\n",
    "\n",
    "# Best Prediction\n",
    "outputs = model(tokenized_1).logits\n",
    "# outputs = model(tmp).logits\n",
    "# outputs = model(encoded_datasets['train']).logits\n",
    "\n",
    "# print(outputs.shape)\n",
    "answer = np.argmax(outputs)\n",
    "\n",
    "print('======= Test Case 1 =======')\n",
    "print(f\"Question: {first[0]}\\n\")\n",
    "print(f\"The answer is choice {answer}: {second[answer]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Test Case 2 =======\n",
      "Question: Which of the following is an accurate definition of dynamic scaling in self-similar systems?\n",
      "\n",
      "The answer is choice 2: Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y.\n"
     ]
    }
   ],
   "source": [
    "first = first_sentence2\n",
    "second = second_sentence2\n",
    "\n",
    "# tokenized = tokenizer(full_sentences, padding=\"longest\", return_tensors=\"np\")\n",
    "tokenized = tokenizer(first, second, padding=\"longest\", return_tensors=\"np\")\n",
    "tokenized_2 = {key: np.expand_dims(array, 0) for key, array in tokenized.items()}\n",
    "\n",
    "# Best Prediction\n",
    "outputs = model(tokenized_2).logits\n",
    "# outputs = model(tmp).logits\n",
    "# outputs = model(encoded_datasets['train']).logits\n",
    "\n",
    "# print(outputs.shape)\n",
    "answer = np.argmax(outputs)\n",
    "\n",
    "print('======= Test Case 2 =======')\n",
    "print(f\"Question: {first[0]}\\n\")\n",
    "print(f\"The answer is choice {answer}: {second[answer]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Test Case 1 =======\n",
      "Top 5 choices with probabilities:\n",
      "Question: Which of the following is an accurate definition of dynamic scaling in self-similar systems?\n",
      "\n",
      "1. Choice 4 (Proba - 0.20065): Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is independent of the respective data taken from snapshots of any earlier or later time. This independence is tested by a certain time-dependent stochastic variable z. \n",
      "2. Choice 0 (Proba - 0.20019): Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x. \n",
      "3. Choice 2 (Proba - 0.20002): Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y. \n",
      "4. Choice 1 (Proba - 0.19967): Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is similar to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x. \n",
      "5. Choice 3 (Proba - 0.19948): Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y. \n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "top_n =  5 # Set the number of top options you want to display\n",
    "\n",
    "outputs = model(tokenized_1).logits\n",
    "logits = outputs[0]\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "# Get the indices of the top N probabilities in descending order\n",
    "top_indices = np.argsort(probabilities)[::-1][:top_n]\n",
    "\n",
    "print('======= Test Case 1 =======')\n",
    "print(\"Top\", top_n, \"choices with probabilities:\")\n",
    "print(f\"Question: {first[0]}\\n\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i + 1}. Choice {idx} (Proba - {probabilities[idx]:.5f}): {second[idx]} \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Test Case 2 =======\n",
      "Top 5 choices with probabilities:\n",
      "Question: Which of the following is an accurate definition of dynamic scaling in self-similar systems?\n",
      "\n",
      "1. Choice 2 (Proba - 0.20026): Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y. \n",
      "2. Choice 3 (Proba - 0.20014): Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is dissimilar to the respective data taken from snapshots of any earlier or later time. This dissimilarity is tested by a certain time-independent stochastic variable y. \n",
      "3. Choice 1 (Proba - 0.19990): Dynamic scaling refers to the non-evolution of self-similar systems, where data obtained from snapshots at fixed times is similar to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x. \n",
      "4. Choice 4 (Proba - 0.19989): Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times is independent of the respective data taken from snapshots of any earlier or later time. This independence is tested by a certain time-dependent stochastic variable z. \n",
      "5. Choice 0 (Proba - 0.19981): Dynamic scaling refers to the evolution of self-similar systems, where data obtained from snapshots at fixed times exhibits similarity to the respective data taken from snapshots of any earlier or later time. This similarity is tested by a certain time-dependent stochastic variable x. \n"
     ]
    }
   ],
   "source": [
    "outputs = model(tokenized_2).logits\n",
    "logits = outputs[0]\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "# Get the indices of the top N probabilities in descending order\n",
    "top_indices = np.argsort(probabilities)[::-1][:top_n]\n",
    "\n",
    "print('======= Test Case 2 =======')\n",
    "print(\"Top\", top_n, \"choices with probabilities:\")\n",
    "print(f\"Question: {first[0]}\\n\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i + 1}. Choice {idx} (Proba - {probabilities[idx]:.5f}): {second[idx]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
